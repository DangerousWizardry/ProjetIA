{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.cluster.hierarchy as sch\n",
    "from sklearn.cluster import DBSCAN\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('data/wb_dataset.txt', sep='\\t',header=0)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mortalité par cancer en fonction des dépenses en santé du pays, des dépenses directes de chaque citoyen en santé, du PIB et de l'indice de capital humain (World Bank Indice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attribute selection\n",
    "===================\n",
    "\n",
    "* Remove attributes containing many missing values.\n",
    "\n",
    "* Remove attributes that seem redundant because of strong correlations.\n",
    "\n",
    "* Used derived attributes to exhibit variables that could be more pertinent (relative value, ratio, difference, relative variation, indicator combining several attributes, ...).\n",
    "\n",
    "* Choose a subset of the dimensions to focus on some aspects and/or to simplify the interpretations. For methods that use distances, reduce the number of dimensions between 4 and 6, and standardize the data if necessary.\n",
    "\n",
    "Remark: It is possible to complete the dataset with other sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Renaming attribute for easier use\n",
    "# Time = removed\n",
    "# Time Code = removed\n",
    "# Country Name = country_name\n",
    "# Country Code = country_code\n",
    "# Mortality from CVD, cancer, diabetes or CRD between exact ages 30 and 70 (%) = mortality\n",
    "# Current health expenditure (% of GDP) [SH.XPD.CHEX.GD.ZS] = health_cost_percent\n",
    "# Current health expenditure per capita (current US$) [SH.XPD.CHEX.PC.CD] = health_cost_capita_percent\n",
    "# Out-of-pocket expenditure (% of current health expenditure) [SH.XPD.OOPC.CH.ZS] = health_oop_cost\n",
    "# GDP (current US$) [NY.GDP.MKTP.CD] = gdp\n",
    "# GDP per capita (current US$) [NY.GDP.PCAP.CD] = gdp_capita\n",
    "# Human capital index (HCI) (scale 0-1) [HD.HCI.OVRL] hci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop the 2 first column (time and time code)\n",
    "data.drop([\"Time\",\"Time Code\"], axis=1, inplace=True)\n",
    "# Remove the last 5 lines that are now empty\n",
    "data.drop(data.tail(5).index,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns = ['country_name','country_code','mortality','health_cost_percent','health_cost_capita_percent','health_oop_cost','gdp','gdp_capita','hci']\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hcp=pd.to_numeric(data[data[\"health_cost_percent\"]!=\"..\"][\"health_cost_percent\"],downcast='float')\n",
    "hccp=pd.to_numeric(data[data[\"health_cost_capita_percent\"]!=\"..\"][\"health_cost_capita_percent\"],downcast='float')\n",
    "np.corrcoef(hcp,hccp) #il semble y avoir une corrélation entre health_cost_percent et health_cost_capita_percent\n",
    "#On choisit donc de supprimer la variable health_cost_capita_percent      ###Si tu confirmes mdrr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdp = pd.to_numeric(data[data[\"gdp\"]!=\"..\"][\"gdp\"],downcast='float')\n",
    "gdpc = pd.to_numeric(data[data[\"gdp_capita\"]!=\"..\"][\"gdp_capita\"],downcast='float')\n",
    "np.corrcoef(gdp,gdpc) #il ne semble pas y avoir une corrélation très importante entre gdp et gdp_capita"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Object selection\n",
    "================\n",
    "\n",
    "* Remove objects containing missing values (except if using methods that handle clearly the missing values).\n",
    "\n",
    "* Identified the outliers (exceptional objects, noise, ...) in 1D, 2D, n-dimensions. Keep track of them and eventually remove them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We remove the region object because they are already agregate of data which we are not interested into\n",
    "count_removed_region = len(data) - data.index[data['country_code'] == \"ZWE\"][0] - 1\n",
    "if count_removed_region > 0 :\n",
    "    data.drop(data.tail(count_removed_region).index,inplace=True)\n",
    "    \n",
    "print(count_removed_region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We remove the object where we have no data about cancers mortality\n",
    "count_removed_nodata = len(data.loc[data['mortality'] == \"..\"])\n",
    "if (count_removed_nodata > 0):\n",
    "    data.drop(data.index[data['mortality'] == \"..\"],inplace=True)\n",
    "\n",
    "print(count_removed_nodata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We choose to remove the object where at least one of the selected attributes are missing\n",
    "missing_attributes = pd.DataFrame(data.apply(pd.Series.value_counts, axis=1)[\"..\"])\n",
    "missing_attributes_index = missing_attributes.index[missing_attributes[\"..\"] > 0]\n",
    "if (len(missing_attributes_index)>0):\n",
    "    data.drop(missing_attributes_index,inplace=True)\n",
    "print(len(missing_attributes_index))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.to_numeric(data[data[\"mortality\"] != \"..\"][nomColonne], downcast='float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[['mortality','health_cost_percent','health_cost_capita_percent','health_oop_cost','gdp','gdp_capita','hci']] = data[['mortality','health_cost_percent','health_cost_capita_percent','health_oop_cost','gdp','gdp_capita','hci']].apply(pd.to_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Looking to the outliers\n",
    "classes=pd.cut(data[\"mortality\"],5, labels=[\"low\",\"medium\", \"average\", \"high\",\"very_high\"])\n",
    "full_clean_dataset[\"mortality_class\"] = classes\n",
    "outliers_index = full_clean_dataset.index[full_clean_dataset[\"mortality_class\"]==\"very_high\"]\n",
    "data.drop(outliers_index,inplace=True)\n",
    "outliers_index = full_clean_dataset.index[full_clean_dataset[\"mortality_class\"]==\"high\"]\n",
    "data.drop(outliers_index,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_clean_dataset = data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Clustering\n",
    "==========\n",
    "\n",
    "* Look for clusters of globular shapes and of arbitrary shapes, using in particular K-means, hierarchical complete/single and DBSCAN.\n",
    "\n",
    "* Compute dendrograms for hierarchical clustering.\n",
    "\n",
    "* Determine good candidates for the number of clusters (using SSE, silhouette coefficient and grouping distance curves).\n",
    "\n",
    "* Study the stability of the K-means convergence.\n",
    "\n",
    "* Compare (using entropy or mutual entropy, and contingency tables) the content of the clusters to a known labelling or to the result of another clustering.\n",
    "\n",
    "* Describe the envelope (the borders) of the clusters using a decision tree (on a dataset having at least 4 dimensions).\n",
    "\n",
    "\n",
    "Remarks:\n",
    "\n",
    "- Removing outliers can improve the stability and the dispersion.\n",
    "\n",
    "- Clustering evaluation can be made by comparing SSE and silhouette coefficient obtain of the data to their values on random dataset or on partially randomize data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hierachical clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's define X and Y for Hierachical Clustering\n",
    "#Y=pd.cut(full_clean_dataset[\"mortality\"],5, labels=[\"low\",\"medium\", \"average\", \"high\",\"very_high\"])\n",
    "X=full_clean_dataset[['health_cost_percent','health_cost_capita_percent','health_oop_cost','gdp','gdp_capita','hci']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = sch.linkage(X, method='complete', metric='euclidean',optimal_ordering=True)\n",
    "#Draw the dendrogram:\n",
    "fig = plt.figure(figsize=(20, 40))\n",
    "dendro = sch.dendrogram(Z, orientation='left', leaf_rotation=0, leaf_font_size=15,labels=list(full_clean_dataset['country_name']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 7))\n",
    "plt.plot(Z[:,2],'o-')\n",
    "plt.grid(axis='y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used complete hierachical clustering. By looking at the curving distance curves, it seems that 6 groups can be highlighted. We'll test this method with 6 clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DBScan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objects=full_clean_dataset[['health_cost_percent','health_cost_capita_percent','health_oop_cost','gdp','gdp_capita','hci']]\n",
    "#We must scaled and center data in order to use efficiently dbscan\n",
    "objects = (objects - objects.mean()) / (objects.max() - objects.min())\n",
    "unclassified = []\n",
    "nb_cluster = []\n",
    "for i in range(0,100):\n",
    "    dbscan = DBSCAN(eps=(0.10 + (i*0.001)), min_samples=2)\n",
    "    dbscan.fit(objects)\n",
    "    unclassified.append(np.count_nonzero(dbscan.labels_ == -1))\n",
    "    nb_cluster.append(max(dbscan.labels_))\n",
    "plt.plot(list(range(0,100)),unclassified)\n",
    "plt.plot(list(range(0,100)),nb_cluster)\n",
    "plt.axhline(y = len(objects)*0.2, color = 'r', linestyle = '-',label=\"20% of value is unclassified\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want at least 80% of our value to be classified, we must choose eps ~>0.136, we choose eps = 0.145"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dbscan = DBSCAN(eps=0.145, min_samples=2)\n",
    "dbscan.fit(objects)\n",
    "dbscan.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cluster = dbscan.labels_\n",
    "objects['cluster']= cluster\n",
    "no_noise_objects = objects[objects['cluster']!=-1]\n",
    "sns.pairplot(data=no_noise_objects,hue='cluster')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DBScan seems to have difficulty to differenciate clusters. It seems that there is a lot of noise in his prediction. It can maybe be caused by scaling and centering data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get SSE when varying the number of clusters\n",
    "objects=full_clean_dataset[['health_cost_percent','health_cost_capita_percent','health_oop_cost','gdp','gdp_capita','hci']]\n",
    "sse_list = []\n",
    "for i in range(2,11):\n",
    "    km_i_clusters=KMeans(n_clusters=i)\n",
    "    km_i_clusters.fit(objects)\n",
    "    sse_list = sse_list + [km_i_clusters.inertia_]\n",
    "fig = plt.figure(figsize=(7, 4))\n",
    "plt.plot(k_list,sse_list,'bo--')\n",
    "plt.grid()\n",
    "plt.xlabel(\"nb of clusters\",fontsize=14)\n",
    "plt.ylabel(\"SSE\",fontsize=14)\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By comparing SSE value, 4 and 5 clusters seems to be a nice choice. We decide to choose 5 clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to know if our data is noisy or if the clusters are easily drawn, we study the stability of the K-means convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_stability(km,df,iterations=100):\n",
    "    avg_silhouette_coef = []\n",
    "    sse_list = []\n",
    "    for i in range(100):\n",
    "        km.fit(df)\n",
    "        labels = km.predict(df)\n",
    "        avg_silhouette_coef.append(silhouette_score(df, labels,metric='euclidean'))\n",
    "    avg_silhouette_coef = np.asarray(avg_silhouette_coef)\n",
    "    return(avg_silhouette_coef.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km = KMeans(n_clusters=5,init='random',n_init=1) # create a KMeans object\n",
    "compute_stability(km,objects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variance is very low, whatever the initial point choosen by Kmeans, the convergence clusters seems to be the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km=KMeans(n_clusters=5)\n",
    "result=km.fit(objects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method Comparision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Hierachical Clustering Score - 6 clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = sch.fcluster(Z, 6, criterion='maxclust')\n",
    "classes=pd.cut(full_clean_dataset[\"mortality\"],6, labels=[\"low\",\"medium\", \"average\", \"high\",\"very_high\",\"very_very_high\"])\n",
    "crosstab=pd.crosstab(clusters,classes)\n",
    "sns.heatmap(crosstab, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) DBScan - 5 clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = dbscan.labels_\n",
    "classes=pd.cut(full_clean_dataset[\"mortality\"],5, labels=[\"low\",\"medium\", \"average\", \"high\",\"very_high\"])\n",
    "crosstab=pd.crosstab(cluster,classes)\n",
    "sns.heatmap(crosstab, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Kmeans - 5 clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters=result.labels_\n",
    "classes=pd.cut(full_clean_dataset[\"mortality\"],5, labels=[\"low\",\"medium\", \"average\", \"high\",\"very_high\"])\n",
    "crosstab=pd.crosstab(clusters,classes)\n",
    "sns.heatmap(crosstab, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objects = objects.copy()\n",
    "objects['cluster']= clusters\n",
    "sns.pairplot(data=objects,hue='cluster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objects[objects['cluster']==3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can't find real relation between the different country by using Clustering on our dataset.\n",
    "\n",
    "We can only spot 3 tendencies\n",
    "\n",
    "* A first group of country look globally similar. It contains Australia, France, Italy, Korea, Canada for instance. (cluster 2)\n",
    "\n",
    "* A second group of country which contains only Germany and Japan which seems to be really cost-efficient in decreasing mortality\n",
    "\n",
    "* Two specific country considered as alone in their cluster because of their specificity : China and USA \n",
    "    * China has an very high GPD but an average health system\n",
    "    * USA has a very expensive health system but not cost efficient at all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification\n",
    "==============\n",
    "\n",
    "* Construct a label by discretisation of an attribute (this label can be built by clustering the values of this attribute). Use this label as class label.\n",
    "\n",
    "* Compare the results obtained using decision trees and the K nearest neighbors.\n",
    "\n",
    "* Evaluate the quality of the model using cross validation. Report the score for each subset and the global score.\n",
    "\n",
    "* Modify the learning parameters to detect of possible overfitting.\n",
    "\n",
    "\n",
    "Remarks:\n",
    "\n",
    "- A contingency table can be use to analyse the errors by class.\n",
    "\n",
    "- Removing outliers can reduce error.\n",
    "\n",
    "- A classification model can be use to predict labels of a targeted attribute for objects where this attribute value is missing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ split the dataset into a train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=data.copy()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ create a Decision Tree instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report content and format\n",
    "=========================\n",
    "\n",
    "* Keep track of the choices made and justify them.\n",
    "\n",
    "* Give commands and parameters so that the results can be reproduced.\n",
    "\n",
    "* Show in tables some views of subsets of the data, but not the complete view of all objects in the data. Give the attributes and their units, as well as the number of objects used.\n",
    "\n",
    "* Give results (values, graphics,...).\n",
    "\n",
    "* Try to interpret the results in the domain.\n",
    "\n",
    "\n",
    "Remarks:\n",
    "\n",
    "- It is possible to study several subsets of attributes (e.g., one for the clustering part, another for the classification tasks).\n",
    "\n",
    "- The report can suggest directions for future works, e.g., directions that have not been explored due to time constraints.\n",
    "\n",
    "- The document can be split in two reports: one for the clustering and one for the classification tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to edit the report\n",
    "======================\n",
    "\n",
    "A way to prepare the report is to add \"Markdows\" cells in the Jupyter notebook to insert text, and use #, ##, ###, #### for headings (section, subsection, subsubsection, paragraph). Next, to get a latex version of the notebook use: \"File -> Download as -> LaTeX\" (this requires Pandoc to be installed https://pandoc.org/installing.html). And then edit the .tex if needed (to add a title page, to clean some parts, ...), before compiling it.\n",
    "\n",
    "IMPORTANT 1: the end of long lines in cells containing python commands can be suppressed (due to the latex verbatim mode), to avoid this use lines of at most 80 characters and use \"\\\" to continue the command on the next line.\n",
    "\n",
    "IMPORTANT 2: if using \"File -> print preview\" to generate a pdf of the notebook, then the sections will not be numbered, and check also that there is no missing part in long lines.\n",
    "\n",
    "IMPORTANT 3: \"File -> Download as -> LaTeX\" may not work for the graphical representation of decision trees, depending on the version of sklearn and on external installed softwares. A workaround is to generate the pdf of the tree and then to include the pdf by a latex command.\n",
    "Example, with a cell containing the code:\n",
    "graph = graphviz.Source(dot_data)\n",
    "graph # to display the tree\n",
    "\n",
    "replace the cell content by:\n",
    "graph = graphviz.Source(dot_data)\n",
    "graph.render(\"out_iris_decision_tree\") # to generate a pdf file\n",
    "\n",
    "then just below this cell add a Markdown cell containing the following tree lines:\n",
    "\\begin{center}\n",
    "\\adjustimage{max size={0.9\\linewidth}{0.9\\paperheight}}{out_iris_decision_tree.pdf}\n",
    "\\end{center}\n",
    "\n",
    "then generate the latex file with \"File -> Download as -> LaTeX\", put the file out_iris_decision_tree.pdf in the folder of the latex file, and compile the latex file.\n",
    "\n",
    "---\n",
    "\n",
    "Zip file to be send by mail\n",
    "===========================\n",
    "(mail to Christophe.Rigotti@insa-lyon.fr and Sergio.Peignier@insa-lyon.fr)\n",
    "\n",
    "Prepare a single folder with name the names of the authors (in lexicographic order): NAME1_NAME2. Zip this folder and send the file NAME1_NAME2.zip\n",
    "\n",
    "The folder must contains:\n",
    "\n",
    "- zip file(s) of the data file(s), (txt format to reproduce the work). \n",
    "\n",
    "- file(s), (txt format or pdf) containing the definitions of the variables given by the data provider.\n",
    "\n",
    "- the Jupyter notebook(s) (format .ipynb to reproduce the work).\n",
    "\n",
    "- the report in one or two pdf files.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
